# Keras_learning
## CNN的介绍
CNN是一种自动化提取特征的机器学习模型。首先我们介绍CNN所用到一些基本结构单元：

1.1卷积层：在卷积层中，有一个重要的概念：权值共享。我们通过卷积核与输入进行卷积运算。通过下图可以理解如何进行卷积运算。卷积核从左到右对输入进行扫描，每次滑动1格（步长为1），下图为滑动一次后，卷积核每个元素和输入中绿色框相应位置的元素相乘后累加，得到输出中绿色框中的0。一般会使用多个卷积核对输入数据进行卷积，得到多个特征图。

1.2激活层：对卷积层的输出进行一个非线性映射，因为卷积计算是一种线性计算。常见的激活函数有relu、tanh、sigmoid等，一般使用relu。
1.2.1引入非线性激活函数的原因：
如果不引入非线性映射的话，无论有多少层神经网络，输出都是输入的线性组合，这与一层隐藏层的效果相当。
1.2.2一般使用relu的原因：
在反向传播计算梯度中，使用relu求导明显会比tanh和sigmoid简单，可以减少计算量。
同时，使用tanh和sigmoid，当层数较多时容易导致梯度消失，因为tanh和sigmoid的导数均小于1（可参考激活函数的导数公式），当我们神经网络有多层的时候，每层都要乘以这个小于1的导数，就有可能接近于0，这就是所谓的梯度消失。而使用relu求导，若输出不为0时，导数均为1，可以有效避免梯度消失问题。
另外，relu还会将小于0的映射为0，使得网络较为稀疏，减少神经元之间的依赖，避免过拟合。

1.3池化层：池化的目的就是减少特征图的维度，减少数据的运算量。池化层是在卷积层之后，对卷积的输出，进行池化运算。池化运算，一般有两种MaxPooling和MeanPooling。选取一个池化窗口（一般为2*2），然后从左往右进行扫描，步长一般为2。如下图MaxPooling操作，选取池化窗口中最大值作为该位置的输出。如：左边绿色方框中四个特征值中，选取最大的6作为输出相应位置的特征值。而MeanPooling则是对于池化窗口中的特征值求平均。

1.4全连接层：主要是对特征进行重新的拟合，减少特征信息的丢失。通过卷积池化操作后得到的是多个特征矩阵，而全连接层的输入为向量，所以在进行全连接层之前，要将多个特征矩阵“压平”为一个向量。

对于CNN的卷积、池化操作，其实很多文章都会详细的介绍，但卷积和池化的意义是什么，很多文章都没有明确给出解释。可能会有人认为卷积和池化可以很大程度的减少权重参数，但只是因为这个原因吗？显然不是的，接下来将讲解CNN是如何实现有效的分类从而理解卷积和池化的意义。

用深度学习解决图像识别问题，从直观上讲是一个从细节到抽象的过程。所谓细节，就是指输入图像的每个像素点，甚至像素点构成的边也可以理解为是细节。假设我们大脑接收到一张动物图，大脑最先反应的是该图的点和边。然后由点和边抽象成各种形状，比如三角形或者圆形等，然后再抽象成耳朵和脸等特征。最后由这些特征决定该图属于哪种动物。深度学习识别图像也是同样的道理。这里关键的就是抽象。何为抽象呢？抽象就是把图像中的各种零散的特征通过某种方式汇总起来，形成新的特征。而利用这些新的特征可更好区分图像类别。如刚才这个例子，点和边就是零散的特征，通过将边进行汇总我们就得到了三角形或圆形等新的特征，同理，将三角形这个特征和一些其他零散的特征汇总成耳朵这个新特征。显而易见，耳朵这个新特征会比三角形特征更利于识别图像。

深度学习正是通过卷积操作实现从细节到抽象的过程。因为卷积的目的就是为了从输入图像中提取特征，并保留像素间的空间关系。何以理解这句话？我们输入的图像其实就是一些纹理，此时，可以将卷积核的参数也理解为纹理，我们目的是使得卷积核的纹理和图像相应位置的纹理尽可能一致。当把图像数据和卷积核的数值放在高维空间中，纹理等价于向量，卷积操作等价于向量的相乘，相乘的结果越大，说明两个向量方向越近，也即卷积核的纹理就更贴近于图像的纹理。因此，卷积后的新图像在具有卷积核纹理的区域信号会更强，其他区域则会较弱。这样，就可以实现从细节（像素点）抽象成更好区分的新特征（纹理）。每一层的卷积都会得到比上一次卷积更易区分的新特征。

而池化目的主要就是为了减少权重参数，但为什么可以以Maxpooling或者MeanPooling代表这个区域的特征呢？这样不会有可能损失了一些重要特征吗？这是因为图像数据在连续区域具有相关性，一般局部区域的像素值差别不大。比如眼睛的局部区域的像素点的值差别并不大，故我们使用Maxpooling或者MeanPooling并不会损失很多特征。
